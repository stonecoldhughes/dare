Project restructuring:
    
    0. Single autotune directory.
    1. gen_config.py makes several different types of executables.
    2. Expand your test program runs dpotrf test too.

Approach:
    
    Modify the saturn test to do a dgemm performance sweep.
    Use these points to find the search space for your script.
    Use those points instead of the ones you have been using.
    See if that improves performance. Try a QR factorization
    instead of Cholesky, because you don't need fake data for QR.

    Start here: investigate which kernels are called for QR, LU, and SVD.
    Plasma calls to look at: dgels, dgeqrf, dtrsm, u

    dgeqrf - qr factorization
    dgetrf - lu factorization
    dtrsm - matrix solve
    dgels - least squares

    if they don't call data dependent kernels... you should totally
    run them instead of Cholesky.

    do a dgemm performance sweep. Find the peak values and use as input
    to the autotuner.

    Move the whole system to saturn. See how it works there. Do this with
    Cholesky first and also QR and the other kernels if your initial step work
    out well.

Solutions:
    
    0. Narrow your point selection by picking local maxima from dgemm performance 
       sweep
    1. Represent results as GFLOPs instead of execution time
    2. Run on Saturn with more cores
    3. Read Clint Whaley's paper. Done, waste of time...
    4. Slowdown likely caused by cache effects. Could your data be placed 
       directly in cache?
       Read the Cholesky algorithm to find out.
    5. Show graphs of performnce vs matrix size. Each curve is a different nb.
       Show crossover points. See if the crossover points match as determined by
       your library.


    flops / second = n^3 / 10^9 / seconds.

    flops = n^3 * iterations.
    
Problems:
    
    0. Prediction is not perfectly accurate. Is it good enough?
    1. Is the core_dpotrf discrepancy to blame for this?
    2. How can I reduce the percentage discrepancy for core_dpotrf?
    3. Something about the fake data is a problem

run.py additions:
    
    0. overall order analysis measure
    1. calculated min and max (top 3)
    2. scripts to analyze kernel_data.txt
       and output data.

    3. Keep in mind, all runs incur library overhead.
       You need to figure out what that overhead is. Have cmake create
       another executable without linkage to the autotune library.
       Add a "tile_size" command line argument and ignore it if it is -1.

    Current test:

        Could it be that your parameters for the autotune library are poorly
        chosen? Could choosing different parameters help? Given the autotune
        counter, numerator, and denominator, maybe not enough actual invocations
        of the function are being called.

        Does the fastest tile_size even stay the same from run to run?

        Could you pick the top couple fastest runs and re-run to find the
        fastest among those?
        
        Your runs are on the order of a few seconds. That's probably
        the smallest allowable runtime. Now start decreasing number of
        iterations. See if that cuts the time a little. The number of
        tile sizes you're iterating over is very very small. Speed it
        up so you can see a wider range of tile sizes. The difference
        between the best and worst case times is very very small.
        
        Run for tile size of 96. Trace and autotune. What is the smallest
        percent difference it can catch?
    
    run.py analysis:
        
        1. Average percentage difference between runtimes
        2. Ordering correctness
    
    kernel_data.txt analysis:
        
        1. Average kernel runtime. Other statistical measurements, such as
           how many of each function type is called. Use this information
           to pick better numerator/denominator values for the autotune library.


Note:

    0. LINKING ORDER MATTERS

Graph the following runs:
    
    0. Unlinked with the autotune library
    1. Linked with the tracer library
    2. Linked with autotune and full execution ratio
    3. With half execution ratio
    4. With quarter execution ratio

    What should the application test? More than one iteration of a Cholesky
    it needs to be able to generate a cholesky matrix of a random size

Current plan:
    
    0. Figure out how your run.py works.
    1. Start running some tests.
    
    what info do you need from this? overall runtime of each application.
    You need each problem type and size with several execution ratios.
    for each execution ratio you need to time each problem size.

    write some more

fake data integration plan:
    
    1. Have an uninitialized pointer to fake data memory.
    2. When it is called for the first time, initialize the whole thing
       and time it. Set the initializing thread.

    Do you really need all those core_blas function pointers to be in autotune? 
    I think so yeah. Autotune will call them instead of profile

Code:

    random_n = rand() ...

    if(random_n < kernel_run)
    {
        if(no_fake_data)
        {
            /* fake_data must track how long it takes to create itself */
            create_fake_data
        }

        if(!clip_empty())
        {
            int count = profile.core_count...
            track_kernel();
            ret_val = ()();
            track_kernel();
            fake_data.append_time(last_kernel_time(opm_get_thread_num()))
        }

        else
        {
            /* Burning time. Track with profile if you need the number */
            fake_data.busy_wait()
        }
    }

How to test it and get numbers:
    
    Show that autotune runs with fake data and 1:1 kernel fraction run approximately
    the same timing-wise as regular profile runs, for many different tile sizes

    Then show that for a sweep of tile sizes, the relative performance is the same.
    Formula to calculate relative error of list ordering:
    Correct list: 1 2 3 4 5
    Incorrect list: 1 3 2 5 4
    Consider 3. It should be preceded by 2 and followed by 4.
    How many spaces would 4 have to move to be after 3? How man would 2 have to move
    to be before 3? The answers are 2 and 2, respectively. The total is 4. Do this
    for each number and the sum is 14. This is the relative error of the orderings.
    For a sweep of 50 tile sizes, order the performance of tile size 0 to 50 for
    a full profiled case and a fake data one. See what the error is. See if it gets
    the same minimum. Time the whole process and see how long it takes.

    run iterations of running through clip cycles of fake_data.
    compare against numbers from plasma runs

    compare execution times of core_dgemms in non-fractional plasma routines
    with core_dgemms in fractional ones.

    also compare error-checking routines like core_dpotrf()

Updated list:

    as soon as a matrix clip is used and the core kernel has been timed, get
    that time off the back of a vector and put it in time_index++ of the clip
    times vector

    once tile returns NULL, you have a clip of times. Busy wait for those
    invocations? What if you interleave actual times and pre-stored times?
    do actual runs dynamically up to a certain size.

    make interleaving configurable parameters. clip_ratio in addition to
    kernel_fraction. The goal is to definitely run the entire clip.
    Higher clip ratios correspond with smaller problems.

    always ignore the first timing result for clip time. Leave it in the
    vector but never randomly select it.

    change the kernel_data map in dare_base.h to an array of pointers to vectors.
    This array has a vector for each thread. Each vector holds a knode.

    embed the new time_kernel within the original, then change name
    after you've verified that the data is the same.

    make track_kernel() a standalone and replace time_kernel with it

Things you must implement:


    You need reloadable matrix clips. An array of matrices to be factored,
    they can all be copies of the same reference one. These will be what
    core_dpotrf randomly factors.

    make tile_size a feature of dare_base?


    Possibly just record the runtimes of each one, and then use this array
    of runtimes to be the sleep time for kernels moving forward? This is only
    if the timing results do not turn out how you expected.

    The ideal solution is to have the clip reload by memcpy()ing the reference
    matrix again into the array during one of the core_blas function calls.
    A core_blas that carries out a reload must consume an equal amount of time
    as a function that only does work. Thus, you must time the memcpy(), then
    afterwards spin long enough to occupy the time slot:

    inline all this:

    t1 = get_time();
    memcpy();
    t2 = get_time();
    memset_time = t2 - t1;

    spin_sleep = kernel_time -  memset_time;

    t = get_time();
    end = t + spin_sleep();

    while(get_time() < end);

    return;

    Each thread needs an instance of its own "fake_data" class for each function
    it may call. That way you don't ever have to worry about thread contention.
    These fake_data objects need to be made part of the autotune classes...

    Another possible solution is just to record the time it takes to reload the
    matrix_clip and subract that from the total calculated runtime. <--- simplest
    solution. Actually no because you don't know how the latencies overlap.
    I suppose you could approximate it by assuming a certain amount of overlap,
    but I don't know how accurate that would be

    create the fake_data class and use it for your timing results

    In hooks.cpp, the usage for the fake data class will roughly be as follows:

    if(fake_data_sets[thread_num].clip_not_empty())
    {
        ret_val = ((core_dpotrf_hook_type)profile.core[CORE_DPOTRF])
        (
         uplo,
         fake_data[thread_num].n,
         fake_data[thread_num].matrix(),
         fake_data[thread_num].lda
        )
    }

    else
    {
        /* carry out reload or sleep protocol */
    }

    or something similar to this

    create a new test directory in which you copy over that file


Mission Statement:

    Create a system to simulate, autotune, and trace libaries
    To create a small problem that is trivial to optimize for.
    This small problem has identical runtime characteristics to the
    problem being autotuned for, but timing results differ only by
    a scalar factor.

Next:
    
    How many different routines do error checking? What kind of parameters do they
    run on? How can I give them correct, dummy data to work on in real time before
    the program starts? What effect do different leading dimension values have
    on runs of mkl core_blas functions? Possibly none, it might only affect memory
    behavior.

    Fake data will passed to the core_blas kernel when it is called.
    It will be constructed as a seperate class object?

    Fake data classes need to be generated as part of a model, all of which inherit
    from the same base class. This is because core_blas potrf uses vastly different
    parameters than core_blas parfb, for example.

    Make autotuning a seperate class and then reintegrate with profile and update
    autogen.py to be able to use it.

    autogen.cpp needs to declare all global objects

    possibly convert global objects to static objects?

    make autotuning stuff and fake data stuff separate static objects
    declared at the end of autogen.cpp. config.xml needs options for max tile
    size. Max tile size will be used for fake data stuff.

    fake data class includes a map that maps kernel type to a pointer to the
    proper static class? Needs a factory method to initialize the class properly.

    put autogeneration warnings at the top of autogenerated files

    ^infrastructure for fake data. Below is the actual research that must
    be done to determine how the fake data should be generated. Does leading
    dimension matter? You should populate an array where each kernel enum
    indexes into the array and the element is a static object of the correct
    fake data type. 
    
    Questions:

        1. How fast are the dummy kernels running compared to the real ones?
        2. Are they slower?
        3. If they are, I need to give them fake data to run.
        4. What parameters are they running with?

    Fields to add:

        elapsed time
        all parameters


    1. Find out what matrix sizes the core_blas functions are working on
    2. Spoof the return value and see if all kernels are still scheduled correctly
    3. See if you can get them all to run artificially
    4. See if you can give them dummy data to run on.
    5. Dummy data needs to be in range of min and max tile sizes.
    6. lapacke_dpotrf() <--- what is the return value?

command string to run main:
    
    ./main 4000 1000 4000 1000 1 250 <<< "2 kernel_fraction 50:50 tile_size 128"

Tasks:

        1. Show that the autotuner approximates real runs by making a graph
           that plots both for the same tile size and increasing problem
           size. Find the window of tile size, problem size, and problem type
           that it approximates the best for. Investigate How small of a
           kernel fraction can be used and still get good results. Make a
           testing script that makes all this easy.


python script:

    run main with a certain tile size and problem size linked to plasma
    run it with same parameters but linked with your library

    passable parameters to main: problem size on command line? m, n, tolerance, iterations

    passable parameters to library: tile size. Pass in on stdin. Set
    tile size with plasma_set() in plasma_init(). kernel fraction? 


Parameters to python script:

    1. Which main executable (determines problem type)
    2. Which tile size
    3. Which problem sizes, maybe as a list
    4. Which kernel fractions


Search space looping order:

   -1. executable
    0. tile size
    1. Iterations of problem size
    2. Problem sizes
    3. For each problem size, use a different kernel fraction.


Random shit to delete:

    1. Write a script to autogenerate parameter files.
       Increase problem size. Keep tile size the same.
       Reduce kernel fraction.
